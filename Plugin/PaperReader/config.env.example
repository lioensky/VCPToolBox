# PaperReader 插件配置（示例）

# === L0 解析层 ===
# MinerU 云端 API Token（从 mineru.net 网站的「个人中心 → API密钥管理」获取）
# 注意：这里填的是 Bearer Token（一串长字符串），不是 Access Key / Secret Key
# 不填则自动降级到 pdf-parse 纯文本模式
MINERU_API_TOKEN=
# MinerU 模型版本：pipeline（默认，速度快）或 vlm（效果更好，速度较慢）
MINERU_MODEL_VERSION=pipeline
# 轮询超时（ms），默认 5 分钟
MINERU_API_TIMEOUT=300000
# 轮询间隔（ms），默认 5 秒
MINERU_POLL_INTERVAL=5000

# === L1 切分层 ===
# 目标 chunk 大小（tokens）
PaperReaderChunkSize=2000
# chunk 重叠比例
PaperReaderOverlap=0.15

# === L2 递归逻辑层 ===
# 读取/总结模型（使用 VCP 的 API_URL/API_Key 调用 /v1/chat/completions）
PaperReaderModel=gemini-2.5-flash-search
# 单次模型输出 token 上限
PaperReaderMaxOutputTokens=12000
# 分批并发组大小（每组处理的 chunk 数，建议 ≤ MaxConcurrentLLM）
# ⚠️ 质量取舍：同批内的 deep chunk 共享同一份 Rolling Context 快照。
#   BatchSize=1（串行）：上下文递进最强，chunk N 能看到 1..N-1 的所有发现
#   BatchSize=5（推荐）：速度与质量的甜蜜点
#   BatchSize=10+：速度最快，但同批 chunk 无法互相感知（skim 不受影响）
#   极高精度需求（法律/财务逐条审计）建议 ≤3
PaperReaderBatchSize=5
# 进程级 LLM 最大并发请求数（防止 429 风暴，建议 3-8）
# 真正的并发控制由此 semaphore 管理，BatchSize 只控制批内共享上下文的范围
PaperReaderMaxConcurrentLLM=5
# deep 阅读最多处理多少个 chunk（防止成本失控）
PaperReaderMaxChunks=120
